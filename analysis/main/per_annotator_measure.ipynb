{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.10 (built by craigm on 2024-08-22 17:33) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets as irds\n",
    "from ir_measures import read_trec_qrels\n",
    "from ir_measures import * \n",
    "from ir_measures import evaluator\n",
    "import pyterrier as pt\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import os\n",
    "from os import path as path\n",
    "if not pt.started(): pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/nfs/primary/annotate/Annotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  analysis  average_docs_per_annotator.png  judgments\tpooling  runs\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vals(run : str, qrels):\n",
    "    original_qrels = pd.DataFrame(irds.load(\"msmarco-passage/trec-dl-2019/judged\").qrels_iter())\n",
    "    original_qrels = original_qrels[original_qrels.query_id.isin(qrels.query_id.unique())]\n",
    "    # Compute the performance per annotator\n",
    "    metrics = [AP(rel=2), NDCG(cutoff=10), R(rel=2)@100, P(rel=2, cutoff=10), RR(rel=2), RR(rel=2, cutoff=10)]\n",
    "\n",
    "    original_evaluate = evaluator(metrics, original_qrels)\n",
    "    evaluate = evaluator(metrics, qrels)\n",
    "\n",
    "    # calculate aggregate performance\n",
    "\n",
    "    original_measures = original_evaluate.calc_aggregate(run)\n",
    "    measures = evaluate.calc_aggregate(run)\n",
    "\n",
    "    original_measures = {str(k) : v for k, v in original_measures.items()}\n",
    "    measures = {str(k) : v for k, v in measures.items()}\n",
    "    \n",
    "    # calculate per query\n",
    "\n",
    "    original_result = {\n",
    "       str(name) : {} for name in metrics\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "         str(name) : {} for name in metrics\n",
    "    }\n",
    "\n",
    "\n",
    "    for metric in original_evaluate.iter_calc(run):\n",
    "        original_result[str(metric.measure)][str(metric.query_id)] = metric.value\n",
    "\n",
    "    for metric in evaluate.iter_calc(run):\n",
    "        result[str(metric.measure)][str(metric.query_id)] = metric.value\n",
    "\n",
    "    # calculate t-test\n",
    "\n",
    "    t_test = {}\n",
    "    old_variance = {}\n",
    "    new_variance = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        original = original_result[str(metric)]\n",
    "        new = {qid : result[str(metric)][qid] for qid in original.keys()}\n",
    "\n",
    "        \n",
    "        t_test[str(metric)] = ttest_ind(list(original.values()), list(new.values())).pvalue\n",
    "        old_variance[str(metric)] = np.var(list(original.values()))\n",
    "        new_variance[str(metric)] = np.var(list(new.values()))\n",
    "        \n",
    "\n",
    "    final = []\n",
    "    for metric in metrics:\n",
    "        final.append(\n",
    "            {\n",
    "                'metric' : str(metric),\n",
    "                'original' : original_measures[str(metric)],\n",
    "                'new' : measures[str(metric)],\n",
    "                'p_value' : t_test[str(metric)],\n",
    "                'original_variance' : old_variance[str(metric)],\n",
    "                'new_variance' : new_variance[str(metric)],\n",
    "            } \n",
    "        )\n",
    "    \n",
    "    return pd.DataFrame(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"msmarco-passage/trec-dl-2019/judged\"\n",
    "qrel_directory = 'judgments/main/qrels/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qrels = {}\n",
    "for file in os.listdir(qrel_directory):\n",
    "    if file.endswith('.txt'):\n",
    "        qrels = pd.DataFrame(read_trec_qrels(qrel_directory + file))\n",
    "        annotator = file.replace('.txt', '').replace('-qrels', '')\n",
    "        all_qrels[annotator] = qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DIR = 'runs/trec-dl-2019'\n",
    "BM25_TUNED = 'dl-19-official-input.bm25tuned_p.gz'\n",
    "BM25_BASE = 'dl-19-official-input.bm25base_p.gz'\n",
    "SET_ENCODER_COLBERT = 'colbert_monoelectra-base_msmarco-passage-trec-dl-2019-judged.run'\n",
    "COLBERT = 'maik-froebe-colbert-run.txt'\n",
    "SPLADE = 'maik-froebe-splade-run.txt'\n",
    "RANK_ZEPHYR = 'maik-froebe-rank-zephyr-run.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_TUNED_RUN = pt.io.read_results(path.join(RUN_DIR, BM25_TUNED)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "BM25_BASE_RUN = pt.io.read_results(path.join(RUN_DIR, BM25_BASE)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "SET_ENCODER_COLBERT_RUN = pt.io.read_results(path.join(RUN_DIR, SET_ENCODER_COLBERT)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "COLBERT_RUN = pt.io.read_results(path.join(RUN_DIR, COLBERT)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "SPLADE_RUN = pt.io.read_results(path.join(RUN_DIR, SPLADE)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "RANK_ZEPHYR_RUN = pt.io.read_results(path.join(RUN_DIR, RANK_ZEPHYR)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "\n",
    "runs = {\n",
    "    'bm25_tuned': BM25_TUNED_RUN,\n",
    "    'bm25_base': BM25_BASE_RUN,\n",
    "    'set_encoder_colbert': SET_ENCODER_COLBERT_RUN,\n",
    "    'colbert': COLBERT_RUN,\n",
    "    'splade': SPLADE_RUN,\n",
    "    'rank_zephyr': RANK_ZEPHYR_RUN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_structure = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/opt/miniconda3/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/opt/miniconda3/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/opt/miniconda3/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/opt/miniconda3/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "for annotator, qrels in all_qrels.items():\n",
    "    for run_name, run in runs.items():\n",
    "        final = get_vals(run, qrels)\n",
    "        final['annotator'] = annotator\n",
    "        final['run_name'] = run_name\n",
    "        final_structure.append(final)\n",
    "\n",
    "final_structure = pd.concat(final_structure)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>original</th>\n",
       "      <th>new</th>\n",
       "      <th>p_value</th>\n",
       "      <th>original_variance</th>\n",
       "      <th>new_variance</th>\n",
       "      <th>annotator</th>\n",
       "      <th>run_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP(rel=2)</td>\n",
       "      <td>0.392135</td>\n",
       "      <td>0.242412</td>\n",
       "      <td>0.175489</td>\n",
       "      <td>0.093514</td>\n",
       "      <td>0.044515</td>\n",
       "      <td>andrew-parry</td>\n",
       "      <td>bm25_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nDCG@10</td>\n",
       "      <td>0.575140</td>\n",
       "      <td>0.344164</td>\n",
       "      <td>0.038261</td>\n",
       "      <td>0.068443</td>\n",
       "      <td>0.064711</td>\n",
       "      <td>andrew-parry</td>\n",
       "      <td>bm25_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R(rel=2)@100</td>\n",
       "      <td>0.546917</td>\n",
       "      <td>0.535538</td>\n",
       "      <td>0.927941</td>\n",
       "      <td>0.093204</td>\n",
       "      <td>0.092840</td>\n",
       "      <td>andrew-parry</td>\n",
       "      <td>bm25_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P(rel=2)@10</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.223077</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.083905</td>\n",
       "      <td>0.026391</td>\n",
       "      <td>andrew-parry</td>\n",
       "      <td>bm25_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RR(rel=2)</td>\n",
       "      <td>0.887179</td>\n",
       "      <td>0.477137</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>0.070690</td>\n",
       "      <td>0.177377</td>\n",
       "      <td>andrew-parry</td>\n",
       "      <td>bm25_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nDCG@10</td>\n",
       "      <td>0.626019</td>\n",
       "      <td>0.574421</td>\n",
       "      <td>0.626651</td>\n",
       "      <td>0.074097</td>\n",
       "      <td>0.079969</td>\n",
       "      <td>sean-macavaney</td>\n",
       "      <td>rank_zephyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R(rel=2)@100</td>\n",
       "      <td>0.463101</td>\n",
       "      <td>0.530038</td>\n",
       "      <td>0.607040</td>\n",
       "      <td>0.108480</td>\n",
       "      <td>0.123362</td>\n",
       "      <td>sean-macavaney</td>\n",
       "      <td>rank_zephyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P(rel=2)@10</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.256752</td>\n",
       "      <td>0.092622</td>\n",
       "      <td>0.093067</td>\n",
       "      <td>sean-macavaney</td>\n",
       "      <td>rank_zephyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RR(rel=2)</td>\n",
       "      <td>0.724444</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.900380</td>\n",
       "      <td>0.125995</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>sean-macavaney</td>\n",
       "      <td>rank_zephyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RR(rel=2)@10</td>\n",
       "      <td>0.724444</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.900380</td>\n",
       "      <td>0.125995</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>sean-macavaney</td>\n",
       "      <td>rank_zephyr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric  original       new   p_value  original_variance  \\\n",
       "0      AP(rel=2)  0.392135  0.242412  0.175489           0.093514   \n",
       "1        nDCG@10  0.575140  0.344164  0.038261           0.068443   \n",
       "2   R(rel=2)@100  0.546917  0.535538  0.927941           0.093204   \n",
       "3    P(rel=2)@10  0.461538  0.223077  0.020217           0.083905   \n",
       "4      RR(rel=2)  0.887179  0.477137  0.008798           0.070690   \n",
       "..           ...       ...       ...       ...                ...   \n",
       "1        nDCG@10  0.626019  0.574421  0.626651           0.074097   \n",
       "2   R(rel=2)@100  0.463101  0.530038  0.607040           0.108480   \n",
       "3    P(rel=2)@10  0.493333  0.360000  0.256752           0.092622   \n",
       "4      RR(rel=2)  0.724444  0.706667  0.900380           0.125995   \n",
       "5   RR(rel=2)@10  0.724444  0.706667  0.900380           0.125995   \n",
       "\n",
       "    new_variance       annotator     run_name  \n",
       "0       0.044515    andrew-parry   bm25_tuned  \n",
       "1       0.064711    andrew-parry   bm25_tuned  \n",
       "2       0.092840    andrew-parry   bm25_tuned  \n",
       "3       0.026391    andrew-parry   bm25_tuned  \n",
       "4       0.177377    andrew-parry   bm25_tuned  \n",
       "..           ...             ...          ...  \n",
       "1       0.079969  sean-macavaney  rank_zephyr  \n",
       "2       0.123362  sean-macavaney  rank_zephyr  \n",
       "3       0.093067  sean-macavaney  rank_zephyr  \n",
       "4       0.151289  sean-macavaney  rank_zephyr  \n",
       "5       0.151289  sean-macavaney  rank_zephyr  \n",
       "\n",
       "[252 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
