{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets as irds\n",
    "from ir_measures import read_trec_qrels\n",
    "from ir_measures import * \n",
    "from ir_measures import evaluator\n",
    "import pyterrier as pt\n",
    "from scipy.stats import ttest_ind\n",
    "import os\n",
    "from os import path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vals(run : str, qrel_file : str):\n",
    "    original_qrels = pd.DataFrame(irds.load(\"msmarco-passage/trec-dl-2019/judged\").qrels_iter())\n",
    "    qrels = read_trec_qrels(qrel_file)\n",
    "\n",
    "    # Compute the performance per annotator\n",
    "    metrics = [AP(rel=2), NDCG(cutoff=10), R(rel=2)@100, P(rel=2, cutoff=10), RR(rel=2), RR(rel=2, cutoff=10)]\n",
    "\n",
    "    original_evaluate = evaluator(metrics, original_qrels)\n",
    "    evaluate = evaluator(metrics, qrels)\n",
    "\n",
    "    # calculate aggregate performance\n",
    "\n",
    "    original_measures = original_evaluate.calc_aggregate(run)\n",
    "    measures = evaluate.calc_aggregate(run)\n",
    "\n",
    "    # calculate per query\n",
    "\n",
    "    original_result = {\n",
    "       str(name) : {'query_id' : {}} for name in metrics\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "         str(name) : {'query_id' : {}} for name in metrics\n",
    "    }\n",
    "\n",
    "\n",
    "    for metric in original_evaluate.iter_calc(run):\n",
    "        original_result[str(metric)][str(metric.query_id)] = metric.value\n",
    "\n",
    "    for metric in evaluate.iter_calc(run):\n",
    "        result[str(metric)][str(metric.query_id)] = metric.value\n",
    "\n",
    "    # calculate t-test\n",
    "\n",
    "    t_test = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        t_test[str(metric)] = ttest_ind(original_result[str(metric)]['query_id'].values(), result[str(metric)]['query_id'].values()).pvalue\n",
    "    \n",
    "    return original_measures, measures, t_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"msmarco-passage/trec-dl-2019/judged\"\n",
    "qrel_directory = '/home/andrew/Documents/Code/Annotation/judgments/pilot-round-01/qrels/'\n",
    "annotation_directory = '/home/andrew/Documents/Code/Annotation/judgments/pilot-round-01/doccano/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qrels = {}\n",
    "for file in os.listdir(qrel_directory):\n",
    "    if file.endswith('.txt'):\n",
    "        qrels = pd.DataFrame(read_trec_qrels(qrel_directory + file))\n",
    "        annotator = file.replace('.txt', '').replace('-qrels', '')\n",
    "        all_qrels[annotator] = qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DIR = '/home/andrew/Documents/Code/Annotation/runs/trec-dl-2019'\n",
    "BM25_TUNED = 'dl-19-official-input.bm25tuned_p.gz'\n",
    "BM25_BASE = 'dl-19-official-input.bm25base_p.gz'\n",
    "SET_ENCODER_COLBERT = 'colbert_monoelectra-base_msmarco-passage-trec-dl-2019-judged.run'\n",
    "COLBERT = 'maik-froebe-colbert-run.txt'\n",
    "SPLADE = 'maik-froebe-splade-run.txt'\n",
    "RANK_ZEPHYR = 'maik-froebe-rank-zephyr-run.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_TUNED_RUN = pt.io.read_results(path.join(RUN_DIR, BM25_TUNED)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "BM25_BASE_RUN = pt.io.read_results(path.join(RUN_DIR, BM25_BASE)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "SET_ENCODER_COLBERT_RUN = pt.io.read_results(path.join(RUN_DIR, SET_ENCODER_COLBERT)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "COLBERT_RUN = pt.io.read_results(path.join(RUN_DIR, COLBERT)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "SPLADE_RUN = pt.io.read_results(path.join(RUN_DIR, SPLADE)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "RANK_ZEPHYR_RUN = pt.io.read_results(path.join(RUN_DIR, RANK_ZEPHYR)).rename(columns={'qid': 'query_id', 'docno': 'doc_id'})\n",
    "\n",
    "runs = {\n",
    "    'bm25_tuned': BM25_TUNED_RUN,\n",
    "    'bm25_base': BM25_BASE_RUN,\n",
    "    'set_encoder_colbert': SET_ENCODER_COLBERT_RUN,\n",
    "    'colbert': COLBERT_RUN,\n",
    "    'splade': SPLADE_RUN,\n",
    "    'rank_zephyr': RANK_ZEPHYR_RUN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_structure = {\n",
    "    'run': [],\n",
    "    'original_measures': [],\n",
    "    'measures': [],\n",
    "    't_test': [],\n",
    "    'annotator': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RelevanceEvaluator.__init__() got an unexpected keyword argument 'judged_docs_only_flag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m annotator, qrels \u001b[38;5;129;01min\u001b[39;00m all_qrels\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_name, run \u001b[38;5;129;01min\u001b[39;00m runs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 3\u001b[0m         original_measures, measures, t_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_vals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         final_structure[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(run_name)\n\u001b[1;32m      5\u001b[0m         final_structure[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_measures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(original_measures)\n",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m, in \u001b[0;36mget_vals\u001b[0;34m(run, qrel_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compute the performance per annotator\u001b[39;00m\n\u001b[1;32m      6\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [AP(rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), NDCG(cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m), R(rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m@\u001b[39m\u001b[38;5;241m100\u001b[39m, P(rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m), RR(rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), RR(rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m----> 8\u001b[0m original_evaluate \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_qrels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m evaluate \u001b[38;5;241m=\u001b[39m evaluator(metrics, qrels)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# calculate aggregate performance\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ir_measures/providers/base.py:53\u001b[0m, in \u001b[0;36mProvider.evaluator\u001b[0;34m(self, measures, qrels)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluator\u001b[39m(\u001b[38;5;28mself\u001b[39m, measures, qrels) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Evaluator:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprovider not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ir_measures/providers/fallback_provider.py:33\u001b[0m, in \u001b[0;36mFallbackProvider._evaluator\u001b[0;34m(self, measures, qrels)\u001b[0m\n\u001b[1;32m     31\u001b[0m qrels_teed \u001b[38;5;241m=\u001b[39m QrelsConverter(qrels)\u001b[38;5;241m.\u001b[39mtee(\u001b[38;5;28mlen\u001b[39m(provider_measure_pairs))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (provider, provider_measures), qrels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(provider_measure_pairs, qrels_teed):\n\u001b[0;32m---> 33\u001b[0m     evaluators\u001b[38;5;241m.\u001b[39mappend(\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovider_measures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqrels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(evaluators) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluators[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# skip the overhead of FallbackEvaluator if there's only one\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ir_measures/providers/base.py:53\u001b[0m, in \u001b[0;36mProvider.evaluator\u001b[0;34m(self, measures, qrels)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluator\u001b[39m(\u001b[38;5;28mself\u001b[39m, measures, qrels) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Evaluator:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprovider not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ir_measures/providers/pytrec_eval_provider.py:66\u001b[0m, in \u001b[0;36mPytrecEvalProvider._evaluator\u001b[0;34m(self, measures, qrels)\u001b[0m\n\u001b[1;32m     62\u001b[0m qrels \u001b[38;5;241m=\u001b[39m ir_measures\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mQrelsConverter(qrels)\u001b[38;5;241m.\u001b[39mas_dict_of_dict()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Depending on the measure params, we may need multiple invocations of pytrec_eval\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# (e.g., with different rel_level, since it only supports running with 1 rel_level at a time)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m invokers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_invokers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PytrecEvalEvaluator(measures, invokers, qrels)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ir_measures/providers/pytrec_eval_provider.py:182\u001b[0m, in \u001b[0;36mPytrecEvalProvider._build_invokers\u001b[0;34m(self, measures, qrels)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gains \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# Map the gains\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         these_qrels \u001b[38;5;241m=\u001b[39m {qid: {did: gains\u001b[38;5;241m.\u001b[39mget(score, score) \u001b[38;5;28;01mfor\u001b[39;00m did, score \u001b[38;5;129;01min\u001b[39;00m vals\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m qid, vals \u001b[38;5;129;01min\u001b[39;00m these_qrels\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 182\u001b[0m     invokers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mPytrecEvalInvoker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytrec_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthese_qrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjudged_only\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m invokers\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ir_measures/providers/pytrec_eval_provider.py:208\u001b[0m, in \u001b[0;36mPytrecEvalInvoker.__init__\u001b[0;34m(self, pte, qrels, measure_map, rel_level, judged_only)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pte, qrels, measure_map, rel_level, judged_only):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator \u001b[38;5;241m=\u001b[39m \u001b[43mpte\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRelevanceEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmeasure_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjudged_docs_only_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjudged_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeasure_map \u001b[38;5;241m=\u001b[39m measure_map\n",
      "\u001b[0;31mTypeError\u001b[0m: RelevanceEvaluator.__init__() got an unexpected keyword argument 'judged_docs_only_flag'"
     ]
    }
   ],
   "source": [
    "for annotator, qrels in all_qrels.items():\n",
    "    for run_name, run in runs.items():\n",
    "        original_measures, measures, t_test = get_vals(run, qrels)\n",
    "        final_structure['run'].append(run_name)\n",
    "        final_structure['original_measures'].append(original_measures)\n",
    "        final_structure['measures'].append(measures)\n",
    "        final_structure['t_test'].append(t_test)\n",
    "        final_structure['annotator'].append(annotator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
